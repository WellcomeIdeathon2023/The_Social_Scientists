library(shiny)
library(ggplot2)
library(dplyr)
library(lubridate)
library(tidyverse)
library(igraph)
library(Matrix)
library(tidyverse)
library(networkD3)
library(htmlwidgets)
library(visNetwork)
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/code")
source("functions.R")
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/data")
# Load the categories list from the RDS file
categories <- readRDS("categories.rds")
# List all CSV files in the working directory
csv_files <- list.files(pattern = "*.csv")
# Load the CSV datasets into separate objects
for (file in csv_files) {
# Extract the month and year from the file name
month_year <- gsub("data_|.csv", "", file)
# Read the CSV file and assign it to a named object using the month-year as the name
assign(paste0("data_", month_year), read.csv(file))
}
# Define UI
ui <- fluidPage(
tags$head(
tags$style(
HTML("
.custom-image {
width: 600px;
height: 400px;
}
")
)
),
titlePanel("Monthly Data Analysis"),
sidebarLayout(
sidebarPanel(
selectInput(
inputId = "selected_month",
label = "Select a month:",
choices = format(seq(as.Date("2020-01-01"), as.Date("2022-11-01"), by = "month"), format = "%b-%Y"),
selected = format(as.Date("2020-01-01"), format = "%b-%Y")
)
),
mainPanel(
tabsetPanel(
tabPanel(
"Categories",
plotOutput("barplot_categories")
),
tabPanel(
"Sentiment",
plotOutput("sentiment_plot"),
# Add any other output elements specific to the sentiment analysis here
),
tabPanel(
"Co-occurrence",
imageOutput("co_occurrence_plot")
)
)
)
)
)
server <- function(input, output) {
source("functions.R")
# Generate the plots
output$barplot_categories <- renderPlot({
category_counts <- processEntitiesData(input$selected_month)
ggplot(category_counts, aes(x = reorder(Category, -Count), y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(x = "Category", y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
})
output$sentiment_plot <- renderPlot({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Compute the sums every five days
fiveday_sums <- compute_sums_every_five_days(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_vline(
data = high_neg_dates,
aes(xintercept = as.numeric(date_interval)),
linetype = "dashed",
color = "black",
size = 0.5
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative")
) +
theme_minimal()
})
output$co_occurrence_plot <- renderImage({
# Construct the file path for the image
selected_date <- parse_date_time(input$selected_month, "b-y")
image_file <- paste0(format(selected_date, format = "%b_%Y"), ".png")
image_path <- file.path(getwd(), image_file)
# Return a list with the src and alt attributes
list(src = image_path, alt = "Co-occurrence Plot", class = "custom-image")
}, deleteFile = FALSE)
output$negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get the most common hashtags associated with negative sentiment
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$correlated_negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get hashtags with the highest negative correlation
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_correlated_negative_hashtags(data, timeframe_start, timeframe_end)
})
# Helper function to process the data and generate the category_counts
processEntitiesData <- function(selected_month) {
selected_date <- parse_date_time(selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Create an empty dataframe
variable_sums <- colSums(data[, 17:34])
category_counts <- data.frame(Category = names(variable_sums), Count = variable_sums)
return(category_counts)
}
}
# Run the app
shinyApp(ui = ui, server = server)
install.packages("here")
library(here)
setwd(here("code"))
source("functions.R")
library(shiny); runApp('shiny_app.R')
library(shiny); runApp('shiny_app.R')
library(shiny); runApp('shiny_app.R')
# Get the directory of the R script
library(dplyr)
set.seed(42)
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data1 = read.csv("classified.csv")
data4 = read.csv("classified_1001_1500.csv")
data3 = read.csv("classified_750_1001.csv")
data3 = data3[,-1]
data2 = read.csv("classified_501_750.csv")
topics = read.csv("vax_tweets_topic_distribution_5.csv")
# Get the directory of the R script
library(dplyr)
set.seed(42)
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data1 = read.csv("classified.csv")
data4 = read.csv("classified_1001_1500.csv")
data3 = read.csv("classified_750_1001.csv")
data3 = data3[,-1]
data2 = read.csv("classified_501_750.csv")
topics = read.csv("vax_tweets_topic_distribution_6.csv")
#data  goes from 1 - 500
#seems that data2 goes from 500 - 749 - so 500 repeated in data and data4
#data 3 goes from 750 - 1000
#data 4 goes from 1001 - 1500
data4 = data4[,c(1,3)]
data2 = data2[,c(1,3)]
data2 = data2[-1,]
data2 = data2[1:249,]
colnames(data2) = c("X", "misinformation")
colnames(data3) = c("X", "misinformation")
colnames(data4) = c("X", "misinformation")
data2$misinformation <- ifelse(data2$misinformation == 1, "True", "False")
data4$misinformation <- ifelse(data4$misinformation == 2, "True", "False")
tweets = read.csv("vax_tweets_5.csv")
View(topics)
# Get the directory of the R script
library(dplyr)
set.seed(42)
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data1 = read.csv("classified.csv")
data4 = read.csv("classified_1001_1500.csv")
data3 = read.csv("classified_750_1001.csv")
data3 = data3[,-1]
data2 = read.csv("classified_501_750.csv")
topics = read.csv("vax_tweets_topic_distribution_6.csv")
# Get the directory of the R script
library(dplyr)
set.seed(42)
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data1 = read.csv("classified.csv")
data4 = read.csv("classified_1001_1500.csv")
data3 = read.csv("classified_750_1001.csv")
data3 = data3[,-1]
data2 = read.csv("classified_501_750.csv")
topics = read.csv("vax_tweets_topic_distribution_6.csv")
View(topics)
class(topics$date)
topics$date = as.Date(topics$date)
View(topics)
topics = read.csv("vax_tweets_topic_distribution_6.csv")
View(topics)
topics = read.csv("vax_tweets_topic_distribution_6.csv", format = "%d/$m$Y")
topics = read.csv("vax_tweets_topic_distribution_6.csv", format = "%d/$m/$Y")
topics$date = as.Date(topics$date,format = "%d/$m/$Y")
View(topics)
topics = read.csv("vax_tweets_topic_distribution_6.csv")
topics$date = as.Date(topics$date,format = "%d/$m/$y")
View(topics)
topics = read.csv("vax_tweets_topic_distribution_6.csv")
topics = read.csv("vax_tweets_topic_distribution_6.csv")
class(topics$date)
topics$date = as.Date(topics$date,format = "%d/%m/%y")
View(topics)
# Get the directory of the R script
library(dplyr)
set.seed(42)
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data1 = read.csv("classified.csv")
data4 = read.csv("classified_1001_1500.csv")
data3 = read.csv("classified_750_1001.csv")
data3 = data3[,-1]
data2 = read.csv("classified_501_750.csv")
topics = read.csv("vax_tweets_topic_distribution_6.csv")
class(topics$date)
topics$date = as.Date(topics$date,format = "%d/%m/%y")
#data  goes from 1 - 500
#seems that data2 goes from 500 - 749 - so 500 repeated in data and data4
#data 3 goes from 750 - 1000
#data 4 goes from 1001 - 1500
data4 = data4[,c(1,3)]
data2 = data2[,c(1,3)]
data2 = data2[-1,]
data2 = data2[1:249,]
colnames(data2) = c("X", "misinformation")
colnames(data3) = c("X", "misinformation")
colnames(data4) = c("X", "misinformation")
data2$misinformation <- ifelse(data2$misinformation == 1, "True", "False")
data4$misinformation <- ifelse(data4$misinformation == 2, "True", "False")
tweets = read.csv("vax_tweets_5.csv")
topics_subset = topics[,c("X","dominant_topic")]
tweets = merge(topics_subset, tweets, by="X")
tweets$dominant_topic = as.factor(tweets$dominant_topic)
tweets$date = as.Date(tweets$date)
library(lubridate)
tweets <- tweets %>%
mutate(year = year(date),
month = month(date),
day = day(date),
day_of_week = wday(date, label = TRUE))
data = as.data.frame(rbind(data1, data2, data3, data4))
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + subjectivity_text +
subjectivity_description + polarity_description +
Locations + Symptoms + COVID +
Conspiracy +  Masks + origin + day + month + year"))
formula
# Fit a random forest model
model <- glm(formula, data = train_data, family = binomial)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "response")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions)
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "response")
validation$misinformation = pred
pred1 = predict(model, merged_data, type = "response")
merged_data$misinformation = pred1
final_data = rbind(merged_data, validation)
write.csv(final_data, "vax_tweets_6.csv")
# Add misinformation to the monthly datasets
library(dplyr)
final_data <- final_data %>%
select(-starts_with("hashtag_"))
class(final_data$date)
final_data$date = as.Date(final_data$date)
min_date <- min(final_data$date)
max_date <- max(final_data$date)
# Generate monthly final_datasets and write to CSV
date_range <- seq(min_date, max_date, by = "month")
for (i in 1:(length(date_range) - 1)) {
start_date <- date_range[i]
end_date <- date_range[i+1] - 1
subset_name <- paste0("data_", format(start_date, "%b"), "_", format(start_date, "%Y"))
subset_final_data <- final_data[final_data$date >= start_date & final_data$date <= end_date, ]
assign(subset_name, subset_final_data)
write.csv(subset_final_data, file = paste0(subset_name, ".csv"), row.names = FALSE)
}
runApp('~/GitHub/code/shiny_app.R')
