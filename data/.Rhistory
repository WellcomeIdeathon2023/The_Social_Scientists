labs(x = "Category", y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
})
output$sentiment_plot <- renderPlot({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Compute the sums every five days
fiveday_sums <- compute_sums_every_five_days(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_vline(
data = high_neg_dates,
aes(xintercept = as.numeric(date_interval)),
linetype = "dashed",
color = "black",
size = 0.5
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative")
) +
theme_minimal()
})
output$negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get the most common hashtags associated with negative sentiment
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$correlated_negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get hashtags with the highest negative correlation
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_correlated_negative_hashtags(data, timeframe_start, timeframe_end)
})
# Helper function to process the data and generate the category_counts
processEntitiesData <- function(selected_month) {
selected_date <- parse_date_time(selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Create an empty dataframe
variable_sums <- colSums(data[, 17:34])
category_counts <- data.frame(Category = names(variable_sums), Count = variable_sums)
return(category_counts)
}
}
# Run the app
shinyApp(ui = ui, server = server)
runApp('~/GitHub/The_Social_Scientists/The_Social_Scientists/code/shiny_app.R')
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/data")
runApp('~/GitHub/The_Social_Scientists/The_Social_Scientists/code/shiny_app.R')
runApp('~/GitHub/The_Social_Scientists/The_Social_Scientists/code/shiny_app.R')
library(shiny)
library(ggplot2)
library(dplyr)
library(lubridate)
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/data")
source("functions.R")
# Load the categories list from the RDS file
categories <- readRDS("categories (3).rds")
# List all CSV files in the working directory
csv_files <- list.files(pattern = "*.csv")
# Load the CSV datasets into separate objects
for (file in csv_files) {
# Extract the month and year from the file name
month_year <- gsub("data_|.csv", "", file)
# Read the CSV file and assign it to a named object using the month-year as the name
assign(paste0("data_", month_year), read.csv(file))
}
# Define UI
ui <- fluidPage(
titlePanel("Monthly Data Analysis"),
sidebarLayout(
sidebarPanel(
selectInput(
inputId = "selected_month",
label = "Select a month:",
choices = format(seq(as.Date("2020-01-01"), as.Date("2022-11-01"), by = "month"), format = "%b-%Y"),
selected = format(as.Date("2020-01-01"), format = "%b-%Y")
)
),
mainPanel(
tabsetPanel(
tabPanel(
"Categories",
plotOutput("barplot_categories")
),
tabPanel(
"Sentiment",
plotOutput("sentiment_plot"),
# Add any other output elements specific to the sentiment analysis here
)
)
)
)
)
server <- function(input, output) {
source("functions.R")
# Generate the plots
output$barplot_categories <- renderPlot({
category_counts <- processEntitiesData(input$selected_month)
ggplot(category_counts, aes(x = reorder(Category, -Count), y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(x = "Category", y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
})
output$sentiment_plot <- renderPlot({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Compute the sums every five days
fiveday_sums <- compute_sums_every_five_days(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_vline(
data = high_neg_dates,
aes(xintercept = as.numeric(date_interval)),
linetype = "dashed",
color = "black",
size = 0.5
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative")
) +
theme_minimal()
})
output$negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get the most common hashtags associated with negative sentiment
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$correlated_negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get hashtags with the highest negative correlation
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_correlated_negative_hashtags(data, timeframe_start, timeframe_end)
})
# Helper function to process the data and generate the category_counts
processEntitiesData <- function(selected_month) {
selected_date <- parse_date_time(selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Create an empty dataframe
variable_sums <- colSums(data[, 17:34])
category_counts <- data.frame(Category = names(variable_sums), Count = variable_sums)
return(category_counts)
}
}
# Run the app
shinyApp(ui = ui, server = server)
# Load the dataset
data <- read.csv("vax_tweets_with_sentiment_entities_3.csv")
data$date <- as.Date(data$date)
data$entities <- tolower(data$entities)
# Example categories and corresponding rules or dictionaries
categories <- list(
Organizations = c("CDCgov", "CDC", "NHS", "Moderna", "fda", "WHO", "Fox News"),
Locations = c("America", "USA","American", "UK", "NYC", "Europe", "India", "US", "U.S", "Chicago", "Canada", "Australia", "Florida", "Israel", "California", "Ontario", "Russia", "NYC", "London", "Delhi", "Pakistan",
"Japan", "Germany", "South Africa", "CovidUK", "Ireland", "US", "Indian", "USUN", "EU", "COVID19Aus", "IndiaFightsCorona", "Canadian", "Europe", "China", "Africa"),
Symptoms = c("fever", "cough", "LongCovid", "ICU", "fatigue", "shortness of breath", "sore throat", "loss of taste", "headache", "long COVID", "ICU", "Health"),
COVID = c("COVID19", "COVID", "Corona", "Covid19", "COVID-19", "COVID-19 variants", "Delta variant", "Omicron variant", "COVID19", "COVID", "Covid", "Delta", "DELTA", "ALPHA", "Alpha", "alpha", "detla", "omicron", "OMICRON", "Omicron","DeltaVariant"),
Vaccination = c("COVID vaccine", "COVID vaccine", "COVIDVACCINE", "Covaxin", "COVID Vaccine", "COVID-19 vaccine", "VACCINE", "Chicago Vaccination", "Vaccine", "CovidVaccine", "COVIDvaccine", "COVIDVaccine", "BoosterJab", "Covaxin", "vaccine", "GetVaccinated", "booster shot", "get vaccinated", "vaccination", "BoosterJab", "COVID19Vic"),
Politics = c("Trump", "Joe Biden", "JoeBiden", "Biden", "Boris Johnson", "POTUS", "MAGA", "TRUMP", "Johnson",  "Boris", "Don", "BorisJohnson"),
Conspiracy = c("DrFauci", "Fauci", "Ivermectin", "ivermectin", "IVERMECTIN", "BillGates", "Secret"),
Slurs = c("COVIDIOTS", "COVIDIOT", "Sad"),
Masks = c("MaskUp", "WearAMask"),
Miscellaneous = c("Please", "NOT", "WTF", "Sorry", "Hey", "Visit", "THE", "Book","NOT","Everyone", "Anyone", "Black", "COVID19nsw","COVIDãf¼19", "Get", "Sad", "Sorry", "WTF", "Hey", "Book", "Visit", "CovidVic"),
origin = c(
"Wuhan lab leak", "Wuhan", "lab leak", "lab",
"Virus bioweapon", "bioweapon",
"COVID cover-up","cover-up", "cover up",
"Origin conspiracy", "origin",
"Natural virus theory", "Natural virus",
"Virus escape", "escape",
"Lab-created virus", "lab-created",
"Pandemic engineered", "engineered", "engineer",
"Virus manipulation",
"Origin of COVID",
"COVID origins",
"Virus origin",
"Virus release", "release"
),
vaccine_conspiracy = c(
"Vaccine microchipping", "microchipping", "microchip", "chip",
"Vaccine side effects cover-up", "side effects", "side-effects", "effects",
"Vaccine efficacy doubts", "efficacy",
"Vaccine hidden agenda",
"Vaccine conspiracy",
"Vaccine dangers", "mrna", "rna", "dna", "EUA", "Emergency use", "Emergency-Use",
"Vaccine depopulation",
"Vaccine profit motive", "profit", "for-profit", "for profit",
"Vaccine long-term effects", "long-term", "unauthorised", "approval", "approve", "approved",
"COVID vaccine conspiracy",
"Vaccine risks", "risks",
"Vaccine safety concerns", "safety",
"Vaccine skepticism", "skepticism", "skeptic"
),
government = c(
"Government control agenda",
"Data manipulation",
"Secret experiments",
"Government conspiracy",
"Government hidden motives",
"Government cover-up",
"Government propaganda",
"Government censorship",
"Government corruption",
"COVID government conspiracy",
"Government interference",
"Government deceit",
"Government misinformation"
),
pharma = c(
"Big Pharma profit motive", "Pfizer", "AstraZeneca", "Johnson and Johnson", "Johnson & Johnson", "J and J", "Moderna", "Biontech",
"Alternative treatments suppression", "Alternative treatments",
"Vaccine shortcuts", "Pharma",
"Pharmaceutical conspiracy",
"Pharmaceutical cover-up",
"Pharmaceutical secrets",
"Pharmaceutical agenda",
"Pharmaceutical control",
"Pharmaceutical manipulation",
"Pharmaceutical industry corruption", "industry corruption",
"Pharma influence",
"Pharma propaganda",
"Pharma greed"
),
Five_G = c(
"5G conspiracy", "5G", "Towers", "Masts", "5G masts", "5G towers",
"COVID and 5G link", "Cell phone", "Cell tower", "Mobile", "Mobile Phone", "Mobile phone mast",
"EMF radiation effects",
"Wireless technology dangers", "Wireless", "Radiation", "Microwaves",
"5G health risks",
"5G hidden agenda",
"EMF cover-up", "EMF",
"Electromagnetic radiation conspiracy", "Electromagnetic", "Electromagnetic radiation", "EMR",
"Wireless radiation dangers", "Radiation",
"5G health concerns",
"Electromagnetic hypersensitivity", "hypersensitivity",
"EMF radiation risks",
"Wireless radiation hazards"
),
gates = c(
"Bill Gates microchipping", "Billgates", "Billgatesfoundation",
"Depopulation agenda",
"Gates Foundation conspiracy",
"Population control conspiracy",
"Gates vaccination plot",
"Gates hidden motives",
"Gates depopulation theory", "Gates depopulation",
"Bill Gates secret plan",
"Gates Foundation hidden agenda", "Gates foundation", "Bill Gates foundation", "Melinda", "Melinda Gates", "Bill and Melinda", "Bill & Melinda", "Bill and Melinda Gates",
"Bill Gates conspiracy",
"Gates global control",
"Gates population reduction",
"Gates vaccine skepticism", "Gates", "Bill Gates"
),
nwo = c(
"COVID global control",
"One world government conspiracy", "One world government",
"Agenda 21 implications", "Agenda 21", "21",
"Agenda 2030 conspiracy", "Agenda 2030", "2030",
"NWO hidden agenda",
"Global governance plot",
"Globalist control theory", "control theory",
"New world order conspiracy", "NWO conspiracy", "NWO plot",
"World domination plan", "World domination", "domination",
"Global control conspiracy", "Global Control",
"World government agenda", "world government", "UN agenda", "WHO agenda",
"NWO takeover", "NWO", "New World Order", "NewWorldOrder",
"Globalist power grab", "globalist", "power grab", "power-grab"
),
media = c(
"Media manipulation",
"Censorship agenda", "Censorship",
"Fake news", "Fakenews",
"Media control",
"Media bias", "bias", "biased",
"Information suppression",
"Misinformation dissemination", "misinformation",
"Propaganda machine", "propaganda",
"Media cover-up",
"Media conspiracy",
"Censored information", "Censored",
"Media distortion",
"News manipulation", "News", "Media",
"Fox News", "FoxNews", "BBC", "CNN", "Sky News")
)
# Convert categories to lowercase
for (cat_name in names(categories)) {
categories[[cat_name]] <- tolower(categories[[cat_name]])
}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tidyr)
# Create an empty dataframe
entity_category_df <- data.frame(Entity = character(), Category = character(), stringsAsFactors = FALSE)
# Add empty columns for each category
for (cat_name in names(categories)) {
data[[cat_name]] <- 0
}
# Iterate over each row in your data
for (row in 1:nrow(data)) {
entities <- data[row, "entities"]
# Check if the row has any entities
if (entities != "[]") {
# Remove square brackets and split the string into individual entities
entities <- gsub("\\[|\\]|\'","", entities)
entities <- strsplit(entities, ", ")[[1]]
# Remove leading and trailing whitespaces from each entity
entities <- trimws(entities)
# Iterate over each entity
for (entity in entities) {
category <- NA  # Initialize category as NA
# Check which category the entity belongs to
for (cat_name in names(categories)) {
if (entity %in% categories[[cat_name]]) {
category <- cat_name
break
}
}
# Append the entity and category to the dataframe
entity_category_df <- rbind(entity_category_df, data.frame(Entity = entity, Category = category, stringsAsFactors = FALSE))
# Increment the count for the category in the "data" dataset
if (!is.na(category)) {
data[row, category] <- data[row, category] + 1
}
}
}
}
# Replace NA values with "Undefined"
entity_category_df$Category[is.na(entity_category_df$Category)] <- "Undefined"
# Count the occurrences of each category
category_counts <- entity_category_df %>%
group_by(Category) %>%
summarise(Count = n())
saveRDS(categories, "categories")
# Find the minimum and maximum dates
min_date <- min(data$date)
max_date <- max(data$date)
#unpack hashtags
# Load the required packages
library(tidyverse)
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Unpack hashtags and create new columns
data<- data %>%
mutate(hashtags = str_replace_all(hashtags, '\\[|\\]|"', '')) %>%
separate(hashtags, into = paste0("#", 1:max(str_count(df$hashtags, ',')) + 1), sep = ', ', fill = 'right') %>%
mutate(across(starts_with("#"), ~extract_hashtags(.))) %>%
relocate(matches("#"), .after = last_col())
View(data)
# Unpack hashtags and create new columns
data<- data %>%
mutate(hashtags = str_replace_all(hashtags, '\\[|\\]|"', '')) %>%
separate(hashtags, into = paste0("#", 1:max(str_count(data$hashtags, ',')) + 1), sep = ', ', fill = 'right') %>%
mutate(across(starts_with("#"), ~extract_hashtags(.))) %>%
relocate(matches("#"), .after = last_col())
# Unpack hashtags and create new columns
data2 <- data %>%
mutate(hashtags = str_replace_all(hashtags, '\\[|\\]|"', '')) %>%
separate(hashtags, into = paste0("#", 1:max(str_count(data$hashtags, ',')) + 1), sep = ', ', fill = 'right') %>%
mutate(across(starts_with("#"), ~extract_hashtags(.))) %>%
relocate(matches("#"), .after = last_col())
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Unpack hashtags and create new columns for most common 20
data2 <- data %>%
mutate(hashtags = str_replace_all(hashtags, '\\[|\\]|"', '')) %>%
separate(hashtags, into = "hashtags", sep = ', ', fill = 'right') %>%
unnest_tokens(hashtag, hashtags) %>%
count(hashtag, sort = TRUE) %>%
head(20) %>%
pull(hashtag) %>%
mutate(hashtag = paste0("#", hashtag)) %>%
mutate(hashtag = factor(hashtag, levels = unique(hashtag))) %>%
left_join(data %>%
mutate(hashtags = str_replace_all(hashtags, '\\[|\\]|"', '')) %>%
separate(hashtags, into = "hashtags", sep = ', ', fill = 'right') %>%
mutate(across(hashtags, ~extract_hashtags(.))),
by = c("hashtag" = "hashtags")) %>%
pivot_wider(names_from = hashtag, values_from = hashtag, values_fn = length, values_fill = 0) %>%
relocate(starts_with("#"), .after = last_col())
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Convert hashtags column to a list
data$hashtags <- lapply(data$hashtags, function(x) eval(parse(text = x)))
# Function to extract hashtags from a string
# Load the required packages
# Load the required packages
library(tidyverse)
library(tidytext)
library(jsonlite)
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Convert hashtags column from string to list
data$hashtags <- lapply(data$hashtags, function(x) fromJSON(x))
# Unpack hashtags and create new columns
# Load the required packages
library(tidyverse)
library(tidytext)
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Unpack hashtags and create new columns for most common 20
data <- data %>%
mutate(hashtags = str_remove_all(hashtags, '["\\[\\]]')) %>%
separate(hashtags, into = paste0("#", 1:20), sep = ',\\s*', fill = 'right') %>%
mutate(across(starts_with("#"), ~extract_hashtags(.))) %>%
pivot_wider(names_from = starts_with("#"), values_from = starts_with("#"), values_fn = length, values_fill = 0) %>%
relocate(starts_with("#"), .after = last_col())
# Load the required packages
library(tidyverse)
library(tidytext)
# Function to extract hashtags from a string
extract_hashtags <- function(text) {
tags <- str_extract_all(text, "\\\"([^\\\"]+)\\\"")[[1]]
tags
}
# Unpack hashtags and create new columns for most common 20
data <- data %>%
mutate(hashtags = str_remove_all(hashtags, '["\\[\\]]')) %>%
separate_rows(hashtags, sep = ',\\s*') %>%
group_by(hashtags) %>%
tally(sort = TRUE) %>%
top_n(20) %>%
ungroup() %>%
mutate(hashtags = factor(hashtags, levels = hashtags)) %>%
mutate(hashtags = paste0("#", hashtags)) %>%
left_join(data %>%
mutate(hashtags = str_remove_all(hashtags, '["\\[\\]]')) %>%
separate_rows(hashtags, sep = ',\\s*') %>%
mutate(across(hashtags, ~extract_hashtags(.))),
by = c("hashtags")) %>%
pivot_wider(names_from = hashtags, values_from = hashtags, values_fn = length, values_fill = 0) %>%
relocate(starts_with("#"), .after = last_col())
# List all CSV files in the working directory
csv_files <- list.files(pattern = "*.csv")
# Load the CSV datasets into separate objects
for (file in csv_files) {
# Extract the month and year from the file name
month_year <- gsub("data_|.csv", "", file)
# Read the CSV file and assign it to a named object using the month-year as the name
assign(paste0("data_", month_year), read.csv(file))
}
