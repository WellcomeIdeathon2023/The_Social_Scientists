<<<<<<< Updated upstream
most_common_hashtags <- names(top_hashtags)
# Create new variables for the most common hashtags
for (hashtag in most_common_hashtags) {
hashtag_column <- paste0("hashtag_", hashtag)  # Add prefix "hashtag_" to the column name
data[[hashtag_column]] <- ifelse(grepl(hashtag, data$hashtags), 1, 0)
}
write.csv(data, "vax_tweets_5.csv", row.names = FALSE)
View(data)
=======
library(shiny)
library(ggplot2)
library(dplyr)
library(lubridate)
library(tidyverse)
library(igraph)
library(Matrix)
library(tidyverse)
library(networkD3)
library(htmlwidgets)
library(visNetwork)
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/code")
source("functions.R")
setwd("C:/Users/s1723280/Documents/GitHub/The_Social_Scientists/The_Social_Scientists/data")
# Load the categories list from the RDS file
categories <- readRDS("categories.rds")
# List all CSV files in the working directory
csv_files <- list.files(pattern = "*.csv")
# Load the CSV datasets into separate objects
for (file in csv_files) {
# Extract the month and year from the file name
month_year <- gsub("data_|.csv", "", file)
# Read the CSV file and assign it to a named object using the month-year as the name
assign(paste0("data_", month_year), read.csv(file))
}
# Define UI
ui <- fluidPage(
tags$head(
tags$style(
HTML("
.custom-image {
width: 600px;
height: 400px;
}
")
)
),
titlePanel("Monthly Data Analysis"),
sidebarLayout(
sidebarPanel(
selectInput(
inputId = "selected_month",
label = "Select a month:",
choices = format(seq(as.Date("2020-01-01"), as.Date("2022-11-01"), by = "month"), format = "%b-%Y"),
selected = format(as.Date("2020-01-01"), format = "%b-%Y")
)
),
mainPanel(
tabsetPanel(
tabPanel(
"Categories",
plotOutput("barplot_categories")
),
tabPanel(
"Sentiment",
plotOutput("sentiment_plot"),
# Add any other output elements specific to the sentiment analysis here
),
tabPanel(
"Co-occurrence",
imageOutput("co_occurrence_plot")
)
)
)
)
)
server <- function(input, output) {
source("functions.R")
# Generate the plots
output$barplot_categories <- renderPlot({
category_counts <- processEntitiesData(input$selected_month)
ggplot(category_counts, aes(x = reorder(Category, -Count), y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(x = "Category", y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
})
output$sentiment_plot <- renderPlot({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Compute the sums every five days
fiveday_sums <- compute_sums_every_five_days(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_vline(
data = high_neg_dates,
aes(xintercept = as.numeric(date_interval)),
linetype = "dashed",
color = "black",
size = 0.5
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative")
) +
theme_minimal()
})
output$co_occurrence_plot <- renderImage({
# Construct the file path for the image
selected_date <- parse_date_time(input$selected_month, "b-y")
image_file <- paste0(format(selected_date, format = "%b_%Y"), ".png")
image_path <- file.path(getwd(), image_file)
# Return a list with the src and alt attributes
list(src = image_path, alt = "Co-occurrence Plot", class = "custom-image")
}, deleteFile = FALSE)
output$negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get the most common hashtags associated with negative sentiment
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$correlated_negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get hashtags with the highest negative correlation
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_correlated_negative_hashtags(data, timeframe_start, timeframe_end)
})
# Helper function to process the data and generate the category_counts
processEntitiesData <- function(selected_month) {
selected_date <- parse_date_time(selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Create an empty dataframe
variable_sums <- colSums(data[, 17:34])
category_counts <- data.frame(Category = names(variable_sums), Count = variable_sums)
return(category_counts)
}
}
# Run the app
shinyApp(ui = ui, server = server)
install.packages("here")
library(here)
setwd(here("code"))
source("functions.R")
library(shiny)
library(ggplot2)
library(dplyr)
library(lubridate)
library(tidyverse)
library(igraph)
library(Matrix)
library(tidyverse)
library(networkD3)
library(htmlwidgets)
library(visNetwork)
library(here)
>>>>>>> Stashed changes
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
<<<<<<< Updated upstream
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(data), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin + vaccine_conspiracy + government + pharma + Five_G + gates + nwo + media", paste(hashtag_columns, collapse = "+")))
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(data), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
View(model)
formula
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", hashtag_columns, ")", collapse = "+")))
formuka
formula
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", paste(hashtag_columns, collapse = "+"), ")", sep = ""), collapse = " "))
formula
# Fit a random forest model
model <- randomForest(formula, data = train_data)
View(train_data)
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", paste(hashtag_columns, collapse = "+"), ")", sep = ""), collapse = " "))
formula
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste("as.factor(", paste(hashtag_columns, collapse = " + "), ")", sep = ""), collapse = " + "))
formula
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste(hashtag_columns, collapse = " + "), collapse = " + "))
formula
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste(hashtag_columns, collapse = " + "), collapse = " + "))
formula
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
View(tweets)
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + subjectivity_text + subjectivity_description + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste(hashtag_columns, collapse = " + "), collapse = " + "))
formula
# Fit a random forest model
model <- randomForest(formula, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "prob")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions[,2])
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
# Fit a random forest model
model <- glm(formula, data = train_data, family = binomial)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "response")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions)
# Get the directory of the R script
script_dir <- dirname(rstudioapi::getSourceEditorContext()$path)
# Set the working directory
setwd(file.path(script_dir, "../data"))
data = read.csv("classified.csv")
tweets = read.csv("vax_tweets_5.csv")
summary(tweets$X)
# Check for missing numbers in the "X" column
missing_numbers <- setdiff(1:100000, unique(tweets$X))
merged_data <- merge(data, tweets, by = "X")
merged_data$misinformation = as.factor(merged_data$misinformation)
# Assuming you have a merged dataset called merged_data with columns "misinformation" and other variables
# Set a random seed for reproducibility
set.seed(42)
# Split the data into training and testing sets using an 80:20 ratio
train_indices <- sample(nrow(merged_data), 0.8 * nrow(merged_data))
train_data <- merged_data[train_indices, ]
test_data <- merged_data[-train_indices, ]
# Load the randomForest package
library(randomForest)
# Get the column names that start with "hashtag_"
hashtag_columns <- grep("^hashtag_", colnames(tweets), value = TRUE)
# Create the formula for random forest, including the hashtag columns
formula <- as.formula(paste("misinformation ~ polarity_text + subjectivity_text + subjectivity_description + polarity_description + Organizations + Locations + Symptoms + COVID + Vaccination + Politics +
Conspiracy + Slurs + Masks + Miscellaneous + origin +
vaccine_conspiracy + government + pharma + Five_G +
gates + nwo + media +", paste(hashtag_columns, collapse = " + "), collapse = " + "))
formula
# Fit a random forest model
model <- glm(formula, data = train_data, family = binomial)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "response")
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, predictions)
# Plot the ROC curve
plot(roc_obj, main = "ROC Curve", print.auc = TRUE)
# Add labels and legend
legend("bottomright", legend = paste("AUC =", round(auc(roc_obj), 2)), cex = 0.8)
# Load the pROC package
library(pROC)
# Compute the ROC curve
roc_obj <- roc(test_data$misinformation, prob_predictions)
# Find the threshold that maximizes Youden's Index
max_index <- which.max(roc_obj$sensitivities + roc_obj$specificities - 1)
threshold <- roc_obj$thresholds[max_index]
# Print the threshold that maximizes Youden's Index
print(paste("Threshold maximizing Youden's Index:", threshold))
# Find the observations in 'tweets' that do not have matching values in 'data$X'
validation <- anti_join(tweets, data, by = "X")
pred = predict(model, validation, type = "prob")
validation$misinformation = pred[,2]
# Apply thresholding to convert probabilities to binary predictions
validation$misinformation <- ifelse(validation$misinformation >= threshold, 1, 0)
# Print the updated 'data' dataframe with the binary predictions
summary(as.factor(validation$misinformation))
summary(as.factor(data$misinformation))
=======
#setwd("D:/Programming/Projects/The_Social_Scientists/The_Social_Scientists/data")
# Source the "functions.R" script
source("functions.R")
# Load the categories list from the RDS file
categories <- readRDS("categories.rds")
# List all CSV files in the working directory
csv_files <- list.files(pattern = "*.csv")
# Load the CSV datasets into separate objects
for (file in csv_files) {
# Extract the month and year from the file name
month_year <- gsub("data_|.csv", "", file)
# Read the CSV file and assign it to a named object using the month-year as the name
assign(paste0("data_", month_year), read.csv(file))
}
# Get global dataset to use for global trends tab
global_trends <- read.csv("vax_tweets_5.csv")
# Apply the function to the dataframe
gt2 <- analyze_hashtag(global_trends, "hashtags", "'vaccine'")
# Define UI
ui <- fluidPage(
tags$head(
tags$style(
HTML("
.custom-image {
width: 600px;
height: 400px;
}
")
)
),
titlePanel("Monthly Data Analysis"),
sidebarLayout(
sidebarPanel(
selectInput(
inputId = "selected_month",
label = "Select a month:",
choices = format(seq(as.Date("2020-01-01"), as.Date("2022-11-01"), by = "month"), format = "%b-%Y"),
selected = format(as.Date("2020-01-01"), format = "%b-%Y")
)
),
mainPanel(
tabsetPanel(
tabPanel(
"Categories",
plotOutput("barplot_categories")
),
tabPanel(
"Sentiment",
plotOutput("sentiment_plot"),
# Add any other output elements specific to the sentiment analysis here
),
tabPanel(
"Co-occurrence",
imageOutput("co_occurrence_plot")
),
tabPanel(
"All-Time Trends",
plotOutput("alltime_trends"),
),
tabPanel(
"All-Time Hashtag Analysis",
textInput(inputId = "input_hashtag", label = "Enter a hashtag:", value = ""),
plotOutput("hashtag_plot")
),
)
)
)
)
server <- function(input, output) {
# Generate the plots
output$barplot_categories <- renderPlot({
category_counts <- processEntitiesData(input$selected_month)
ggplot(category_counts, aes(x = reorder(Category, -Count), y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(x = "Category", y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
})
output$sentiment_plot <- renderPlot({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Compute the sums every five days
fiveday_sums <- compute_sums_every_one_day(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_vline(
data = high_neg_dates,
aes(xintercept = as.numeric(date_interval)),
linetype = "dashed",
color = "black",
size = 0.5
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative")
) +
theme_minimal()
})
output$co_occurrence_plot <- renderImage({
# Construct the file path for the image
selected_date <- parse_date_time(input$selected_month, "b-y")
image_file <- paste0(format(selected_date, format = "%b_%Y"), ".png")
image_path <- file.path(getwd(), image_file)
# Return a list with the src and alt attributes
list(src = image_path, alt = "Co-occurrence Plot", class = "custom-image")
}, deleteFile = FALSE)
output$negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get the most common hashtags associated with negative sentiment
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$correlated_negative_hashtags_table <- renderTable({
# Load the necessary data (replace with your own data loading code)
selected_date <- parse_date_time(input$selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Get hashtags with the highest negative correlation
timeframe_start <- "2020-01-01"
timeframe_end <- "2020-06-30"
get_correlated_negative_hashtags(data, timeframe_start, timeframe_end)
})
output$alltime_trends <- renderPlot({
# Load the necessary data (replace with your own data loading code)
data <- global_trends
# Compute the sums every five days
fiveday_sums <- compute_sums_every_thirty_days(data)
# Label high periods
fiveday_sums <- label_high_periods(fiveday_sums)
# Create a new data frame for high_neg == TRUE
high_neg_dates <- fiveday_sums %>%
filter(high_neg == TRUE) %>%
select(date_interval,normalized_neg,sum_favourites_neg)
# Plot the sentiment over time
ggplot(fiveday_sums, aes(x = date_interval)) +
geom_line(aes(y = normalized_pos), color = 'blue') +
geom_line(aes(y = normalized_neg), color = 'red') +
geom_point(
data = high_neg_dates,
aes(x = date_interval, y = normalized_neg),
color = "black",
size = 3
) +
geom_text(
data = high_neg_dates,
aes(x = date_interval, y = normalized_neg),
label = "Negative Peak",
vjust = -1,
hjust = 0.2,
size = 3,
color = "black"
) +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Normalized Sentiment Score",
color = "Legend"
) +
scale_x_date(date_breaks = "3 months", date_labels = "%,b %Y") +
scale_color_manual(
values = c("blue", "red"),
labels = c("Positive", "Negative","All")
) +
theme_minimal()
})
output$hashtag_plot <- renderPlot({
if (input$input_hashtag != "") {
hashtag_data <- analyze_hashtag(global_trends, "hashtags", paste0("'",input$input_hashtag,"'"))
# Plot the sentiment over time
ggplot(hashtag_data, aes(x = date_interval)) +
geom_line(aes(y = sum_favourites_pos), color = 'blue') +
geom_line(aes(y = sum_favourites_neg), color = 'red') +
geom_line(aes(y = sum_favourites_all), color = 'black') +
labs(
title = "Normalized Sentiment Over Time",
x = "Date Interval",
y = "Number of Tweets",
color = "Legend"
) +
scale_x_date(date_breaks = "3 months", date_labels = "%,b %Y") +
scale_color_manual(
values = c("blue", "red","black"),
labels = c("Positive", "Negative","All")
) +
theme_minimal()
}
})
# Helper function to process the data and generate the category_counts
processEntitiesData <- function(selected_month) {
selected_date <- parse_date_time(selected_month, "b-y")
file_name <- paste0("data_", format(selected_date, format = "%b_%Y"))
data <- get(file_name)
# Create an empty dataframe
variable_sums <- colSums(data[, 17:34])
category_counts <- data.frame(Category = names(variable_sums), Count = variable_sums)
return(category_counts)
}
}
# Run the app
shinyApp(ui = ui, server = server)
>>>>>>> Stashed changes
